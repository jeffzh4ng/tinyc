use crate::lexer::{Token, TokenType};
use serde::{Deserialize, Serialize};
use std::io;

#[derive(Clone, PartialEq, Serialize, Deserialize, Debug)]
pub struct Program {
    pub main_function: MainFunction,
}

#[derive(Clone, PartialEq, Serialize, Deserialize, Debug)]
pub struct MainFunction {
    pub stmts: Vec<Stmt>,
}

#[derive(Clone, PartialEq, Serialize, Deserialize, Debug)]
pub struct Id(pub String);

#[derive(Clone, PartialEq, Serialize, Deserialize, Debug)]
pub enum Stmt {
    // Continue,
    // Break,
    Asnmt {
        id: Id,
        expr: Box<Expr>,
    },
    AsnmtUpdate {
        op: BinOp,
        expr: Box<Expr>,
    },
    Return(Expr),
    For,
    While,
    // Dowhile,
    // Switch
    If,
    IfEls {
        cond: Box<Expr>,
        then: Box<Stmt>,
        els: Box<Stmt>,
    },
}

#[derive(Clone, PartialEq, Serialize, Deserialize, Debug)]
pub enum Expr {
    // eliminations (operations)
    Var(Id), // eliminates assignment
    LogE {
        op: LogOp,
        l: Box<Expr>,
        r: Box<Expr>,
    },
    BitE {
        op: BitOp,
        l: Box<Expr>,
        r: Box<Expr>,
    },
    RelE {
        op: RelOp,
        l: Box<Expr>,
        r: Box<Expr>,
    },
    BinE {
        op: BinOp,
        l: Box<Expr>,
        r: Box<Expr>,
    },
    UnaryE {
        op: UnaryOp,
        l: Box<Expr>,
    },

    // introductions (operands)
    // Char
    // - sign: Signed/Unsighed
    Int(i128),
    // - sign: Signed/Unsighed
    // - length: Short/Long
    // Float
    // Double
    Str(String),
}

#[derive(Clone, PartialEq, Serialize, Deserialize, Debug)]
pub enum LogOp {
    And,
    Or,
}

#[derive(Clone, PartialEq, Serialize, Deserialize, Debug)]
pub enum BitOp {
    And,
    Or,
    Xor,
}

#[derive(Clone, PartialEq, Serialize, Deserialize, Debug)]
pub enum RelOp {
    Eq,
    Neq,
    And,
    Or,
    LtEq,
    Lt,
    GtEq,
    Gt,
}

#[derive(Clone, PartialEq, Serialize, Deserialize, Debug)]
pub enum BinOp {
    Add,
    Sub,
    Mult,
    Div,
    Mod,
}

#[derive(Clone, PartialEq, Serialize, Deserialize, Debug)]
pub enum UnaryOp {
    Add,
    Sub,
}

pub fn parse(tokens: Vec<Token>) -> Result<Program, io::Error> {
    let main_function = parse_function(tokens)?;
    Ok(Program { main_function })
}

fn parse_function(tokens: Vec<Token>) -> Result<MainFunction, io::Error> {
    let (_, r) = mtch(&tokens, TokenType::KeywordInt)?;
    let (_, r) = mtch(r, TokenType::KeywordMain)?;
    let (_, r) = mtch(r, TokenType::PuncLeftParen)?;
    let (_, r) = mtch(r, TokenType::PuncRightParen)?;
    let (_, r) = mtch(r, TokenType::PuncLeftBrace)?;

    let mut stmts = vec![];
    let mut r0 = r;
    while let Ok((s, r1)) = parse_stmt(r0) {
        stmts.push(s);
        r0 = r1;
    }
    let (_, r) = mtch(r0, TokenType::PuncRightBrace)?;

    if !r.is_empty() {
        // panic?
    }

    Ok(MainFunction { stmts })
}

fn parse_stmt(tokens: &[Token]) -> Result<(Stmt, &[Token]), io::Error> {
    match tokens {
        [] => todo!(),
        [f, r @ ..] => match f.typ {
            TokenType::KeywordInt => {
                let (idt, r) = mtch(r, TokenType::Identifier)?;
                let (_, r) = mtch(r, TokenType::Equals)?;
                let (expr, r) = parse_rel_expr(r)?;

                let (_, r) = mtch(r, TokenType::PuncSemiColon)?;

                Ok((
                    Stmt::Asnmt {
                        id: Id(idt.lexeme.to_owned()),
                        expr: Box::new(expr),
                    },
                    r,
                ))
            }
            TokenType::Identifier => match r {
                [] => todo!(),
                [f, s, r @ ..] => match (f.typ, s.typ) {
                    (TokenType::Plus, TokenType::Equals) => {
                        let (expr, r) = parse_rel_expr(r)?;
                        let (_, r) = mtch(r, TokenType::PuncSemiColon)?;
                        Ok((
                            Stmt::AsnmtUpdate {
                                op: BinOp::Add,
                                expr: Box::new(expr),
                            },
                            r,
                        ))
                    }
                    (TokenType::Minus, TokenType::Equals) => {
                        let (expr, r) = parse_rel_expr(r)?;
                        let (_, r) = mtch(r, TokenType::PuncSemiColon)?;

                        Ok((
                            Stmt::AsnmtUpdate {
                                op: BinOp::Sub,
                                expr: Box::new(expr),
                            },
                            r,
                        ))
                    }
                    (TokenType::Star, TokenType::Equals) => {
                        let (expr, r) = parse_rel_expr(r)?;
                        let (_, r) = mtch(r, TokenType::PuncSemiColon)?;

                        Ok((
                            Stmt::AsnmtUpdate {
                                op: BinOp::Mult,
                                expr: Box::new(expr),
                            },
                            r,
                        ))
                    }
                    (TokenType::Slash, TokenType::Equals) => {
                        let (expr, r) = parse_rel_expr(r)?;
                        let (_, r) = mtch(r, TokenType::PuncSemiColon)?;

                        Ok((
                            Stmt::AsnmtUpdate {
                                op: BinOp::Div,
                                expr: Box::new(expr),
                            },
                            r,
                        ))
                    }
                    _ => todo!(),
                },
                t => todo!(),
            },
            TokenType::KeywordRet => {
                let (expr, r) = parse_rel_expr(r)?;
                let (_, r) = mtch(r, TokenType::PuncSemiColon)?;
                Ok((Stmt::Return(expr), r))
            }
            TokenType::KeywordIf => {
                let (_, r) = mtch(r, TokenType::PuncLeftParen)?;
                let (cond, r) = parse_rel_expr(r)?;
                let (_, r) = mtch(r, TokenType::PuncRightParen)?;
                let (_, r) = mtch(r, TokenType::PuncLeftBrace)?;
                let (then, r) = parse_stmt(r)?;
                let (_, r) = mtch(r, TokenType::PuncRightBrace)?;
                let (_, r) = mtch(r, TokenType::KeywordEls)?;
                let (_, r) = mtch(r, TokenType::PuncLeftBrace)?;
                let (els, r) = parse_stmt(r)?;
                let (_, r) = mtch(r, TokenType::PuncRightBrace)?;

                Ok((
                    Stmt::IfEls {
                        cond: Box::new(cond),
                        then: Box::new(then),
                        els: Box::new(els),
                    },
                    r,
                ))
            }
            t => Err(io::Error::new(
                io::ErrorKind::Other,
                format!("token not recognizable {:?}", t),
            )),
        },
    }
}

fn parse_rel_expr(tokens: &[Token]) -> Result<(Expr, &[Token]), io::Error> {
    let (left, r) = parse_term(tokens)?;

    match r {
        [] => Ok((left, r)),
        r => {
            let mut cur_node = left;
            let mut r = r;

            while let Ok((op, r_temp)) = parse_rel_op(r) {
                let (right, r_temp) = parse_term(r_temp)?;

                cur_node = Expr::RelE {
                    op,
                    l: Box::new(cur_node),
                    r: Box::new(right),
                };

                r = r_temp;
            }

            Ok((cur_node, r))
        }
    }
}

fn parse_term(tokens: &[Token]) -> Result<(Expr, &[Token]), io::Error> {
    let (left, r) = parse_factor(tokens)?;

    match r {
        [] => Ok((left, r)),
        r => {
            let mut cur_node = left;
            let mut r = r;

            while let Ok((op, r_temp)) = parse_term_op(r) {
                let (right, r_temp) = parse_factor(r_temp)?;

                cur_node = Expr::BinE {
                    op,
                    l: Box::new(cur_node),
                    r: Box::new(right),
                };

                r = r_temp;
            }

            Ok((cur_node, r))
        }
    }
}

fn parse_factor(tokens: &[Token]) -> Result<(Expr, &[Token]), io::Error> {
    let (left, r) = parse_atom(tokens)?;

    match r {
        [] => Ok((left, r)),
        r => {
            let mut cur_node = left;
            let mut r = r;

            while let Ok((op, r_temp)) = parse_factor_op(r) {
                let (right, r_temp) = parse_atom(r_temp)?;

                cur_node = Expr::BinE {
                    op,
                    l: Box::new(cur_node),
                    r: Box::new(right),
                };

                r = r_temp;
            }

            Ok((cur_node, r))
        }
    }
}

fn parse_atom(tokens: &[Token]) -> Result<(Expr, &[Token]), io::Error> {
    match tokens {
        [] => todo!(),
        [f, r @ ..] => match f.typ {
            TokenType::Identifier => Ok((Expr::Var(Id(f.lexeme.to_owned())), r)),
            TokenType::LiteralInt => Ok((Expr::Int(f.lexeme.parse().unwrap()), r)),
            t => Err(io::Error::new(
                io::ErrorKind::Other,
                format!("token not recognizable {:?}", t),
            )),
        },
    }
}

fn parse_rel_op(tokens: &[Token]) -> Result<(RelOp, &[Token]), io::Error> {
    match tokens {
        [] => todo!(),
        [f, r @ ..] => match f.typ {
            TokenType::LeftAngleBracket => match r {
                [] => todo!(),
                [s, r @ ..] => match s.typ {
                    TokenType::Equals => Ok((RelOp::LtEq, r)),
                    _ => Ok((RelOp::Lt, &tokens[1..])), // include s
                },
            },
            TokenType::RightAngleBracket => match r {
                [] => todo!(),
                [s, r @ ..] => match s.typ {
                    TokenType::Equals => Ok((RelOp::GtEq, r)),
                    _ => Ok((RelOp::Gt, &tokens[1..])), // include s
                },
            },
            TokenType::Equals => match r {
                [] => todo!(),
                [s, r @ ..] => match s.typ {
                    TokenType::Equals => Ok((RelOp::Eq, r)),
                    t => Err(io::Error::new(
                        io::ErrorKind::Other,
                        format!("token not recognizable {:?}", t),
                    )),
                },
            },
            TokenType::Bang => match r {
                [] => todo!(),
                [s, r @ ..] => match s.typ {
                    TokenType::Equals => Ok((RelOp::Neq, r)),
                    t => Err(io::Error::new(
                        io::ErrorKind::Other,
                        format!("token not recognizable {:?}", t),
                    )),
                },
            },
            TokenType::Amp => match r {
                [] => todo!(),
                [s, r @ ..] => match s.typ {
                    TokenType::Amp => Ok((RelOp::And, r)),
                    t => Err(io::Error::new(
                        io::ErrorKind::Other,
                        format!("token not recognizable {:?}", t),
                    )),
                },
            },
            TokenType::Bar => match r {
                [] => todo!(),
                [s, r @ ..] => match s.typ {
                    TokenType::Bar => Ok((RelOp::Or, r)),
                    t => Err(io::Error::new(
                        io::ErrorKind::Other,
                        format!("token not recognizable {:?}", t),
                    )),
                },
            },
            t => Err(io::Error::new(
                io::ErrorKind::Other,
                format!("token not recognizable {:?}", t),
            )),
        },
    }
}

fn parse_term_op(tokens: &[Token]) -> Result<(BinOp, &[Token]), io::Error> {
    match tokens {
        [] => todo!(),
        [f, r @ ..] => match f.typ {
            TokenType::Plus => Ok((BinOp::Add, r)),
            TokenType::Minus => Ok((BinOp::Sub, r)),
            t => Err(io::Error::new(
                io::ErrorKind::Other,
                format!("token not recognizable {:?}", t),
            )),
        },
    }
}

fn parse_factor_op(tokens: &[Token]) -> Result<(BinOp, &[Token]), io::Error> {
    match tokens {
        [] => todo!(),
        [f, r @ ..] => match f.typ {
            TokenType::Star => Ok((BinOp::Mult, r)),
            TokenType::Slash => Ok((BinOp::Div, r)),
            t => Err(io::Error::new(
                io::ErrorKind::Other,
                format!("token not recognizable {:?}", t),
            )),
        },
    }
}

fn mtch(tokens: &[Token], tt: TokenType) -> Result<(&Token, &[Token]), io::Error> {
    match tokens {
        [] => todo!(),
        [f, r @ ..] => {
            if f.typ == tt {
                // Use an if-guard to compare values
                Ok((f, r))
            } else {
                Err(io::Error::new(
                    io::ErrorKind::Other,
                    format!("expected: {:?} got: {:?}", tt, f),
                ))
            }
        }
    }
}

#[cfg(test)]
mod test_legal_arithmetic {
    use crate::lexer;
    use std::fs;

    const TEST_DIR: &str = "tests/fixtures/din/legal/arithmetic";

    #[test]
    fn lit() {
        let chars = fs::read(format!("{TEST_DIR}/lit.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                Int: 8
        "###);
    }

    #[test]
    fn add() {
        let chars = fs::read(format!("{TEST_DIR}/add.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                BinE:
                  op: Add
                  l:
                    Int: 9
                  r:
                    Int: 10
        "###);
    }

    #[test]
    fn add_multi() {
        let chars = fs::read(format!("{TEST_DIR}/add_multi.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                BinE:
                  op: Add
                  l:
                    BinE:
                      op: Add
                      l:
                        Int: 9
                      r:
                        Int: 10
                  r:
                    Int: 11
        "###);
    }

    #[test]
    fn sub() {
        #[rustfmt::skip]
        let chars = fs::read(format!("{TEST_DIR}/sub.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                BinE:
                  op: Sub
                  l:
                    Int: 88
                  r:
                    Int: 32
        "###);
    }

    #[test]
    fn mult() {
        #[rustfmt::skip]
        let chars = fs::read(format!("{TEST_DIR}/mult.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                BinE:
                  op: Mult
                  l:
                    Int: 9
                  r:
                    Int: 10
        "###);
    }

    #[test]
    fn div() {
        #[rustfmt::skip]
        let chars = fs::read(format!("{TEST_DIR}/div.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                BinE:
                  op: Div
                  l:
                    Int: 100
                  r:
                    Int: 9
        "###);
    }
}

#[cfg(test)]
mod test_legal_arithmetic_precedence {
    use crate::lexer;
    use std::fs;

    const TEST_DIR: &str = "tests/fixtures/din/legal/arithmetic_precedence";

    #[test]
    fn add_associative() {
        let chars = fs::read(format!("{TEST_DIR}/add_associative.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                BinE:
                  op: Add
                  l:
                    BinE:
                      op: Add
                      l:
                        Int: 9
                      r:
                        Int: 10
                  r:
                    Int: 11
        "###);
    }

    #[test]
    fn sub_associative() {
        let chars = fs::read(format!("{TEST_DIR}/sub_associative.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                BinE:
                  op: Sub
                  l:
                    BinE:
                      op: Sub
                      l:
                        Int: 30
                      r:
                        Int: 9
                  r:
                    Int: 10
        "###);
    }

    #[test]
    fn mult_add_precedence() {
        let chars = fs::read(format!("{TEST_DIR}/mult_add_precedence.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                BinE:
                  op: Add
                  l:
                    BinE:
                      op: Mult
                      l:
                        Int: 9
                      r:
                        Int: 10
                  r:
                    Int: 11
        "###);
    }

    #[test]
    fn mult_add_precedence_multi() {
        let chars = fs::read(format!("{TEST_DIR}/mult_add_precedence_multi.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                BinE:
                  op: Add
                  l:
                    BinE:
                      op: Mult
                      l:
                        Int: 9
                      r:
                        Int: 10
                  r:
                    BinE:
                      op: Mult
                      l:
                        Int: 11
                      r:
                        Int: 12
        "###);
    }
}

#[cfg(test)]
mod test_legal_control_flow {
    use crate::lexer;
    use std::fs;

    const TEST_DIR: &str = "tests/fixtures/din/legal/control_flow";

    #[test]
    fn eq() {
        let chars = fs::read(format!("{TEST_DIR}/eq_true.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                RelE:
                  op: Eq
                  l:
                    Int: 9
                  r:
                    Int: 9
        "###);
    }

    #[test]
    fn neq() {
        let chars = fs::read(format!("{TEST_DIR}/neq_true.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                RelE:
                  op: Neq
                  l:
                    Int: 9
                  r:
                    Int: 10
        "###);
    }

    #[test]
    fn and() {
        let chars = fs::read(format!("{TEST_DIR}/and_true.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                RelE:
                  op: And
                  l:
                    Int: 1
                  r:
                    Int: 1
        "###);
    }

    #[test]
    fn or() {
        let chars = fs::read(format!("{TEST_DIR}/or_true.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                RelE:
                  op: Or
                  l:
                    Int: 1
                  r:
                    Int: 1
        "###);
    }

    #[test]
    fn lt() {
        let chars = fs::read(format!("{TEST_DIR}/lt_true.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                RelE:
                  op: Lt
                  l:
                    Int: 9
                  r:
                    Int: 10
        "###);
    }

    #[test]
    fn gt() {
        let chars = fs::read(format!("{TEST_DIR}/gt_true.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Return:
                RelE:
                  op: Gt
                  l:
                    Int: 10
                  r:
                    Int: 9
        "###);
    }

    #[test]
    fn ifels_then() {
        let chars = fs::read(format!("{TEST_DIR}/ifels_then.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - IfEls:
                cond:
                  RelE:
                    op: Lt
                    l:
                      Int: 9
                    r:
                      Int: 10
                then:
                  Return:
                    Int: 0
                els:
                  Return:
                    Int: 1
        "###);
    }
}

#[cfg(test)]
mod test_legal_data_flow {
    use crate::lexer;
    use std::fs;

    const TEST_DIR: &str = "tests/fixtures/din/legal/data_flow";

    #[test]
    fn asnmt() {
        let chars = fs::read(format!("{TEST_DIR}/asnmt.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Asnmt:
                id: x
                expr:
                  Int: 8
            - Return:
                Var: x
        "###);
    }

    #[test]
    fn asnmt_update() {
        let chars = fs::read(format!("{TEST_DIR}/asnmt_update.c"))
            .expect("Should have been able to read the file")
            .iter()
            .map(|b| *b as char)
            .collect::<Vec<_>>();

        let tokens = lexer::lex(&chars);
        let tree = super::parse(tokens).unwrap();
        insta::assert_yaml_snapshot!(tree, @r###"
        ---
        main_function:
          stmts:
            - Asnmt:
                id: n
                expr:
                  Int: 0
            - AsnmtUpdate:
                op: Add
                expr:
                  Int: 10
            - Return:
                Var: n
        "###);
    }
}

// proptest! {
//     #[test]
//     fn doesnt_crash(s in "\\PC*") {
//         let t = Token{ lexeme: s, typ: TokenType::Identifier };
//         let tokens = vec![t];

//         let _ = parse_program(tokens);
//     }
// }
